## 篇章小测
* **问题1: Transformer中的softmax计算为什么需要除以$d_k$​?**
  * 向量$q,k$点积时会改变数的量级，很可能导致在对softmax优化时出现梯度消失的问题。所以要除以$\sqrt{d_k}$，做归一化。
* **问题2: Transformer中attention score计算时候如何mask掉padding位置？**
  * [参考](https://zhuanlan.zhihu.com/p/139595546)：[padding]是为了补齐长度的字符，无实际意义，所以要用mask屏蔽掉这块的信息。方法是计算$attention\_score=QK^T$时，令mask区域的输出为负无穷(-1e9)，经过softmax后就为0了。
* **问题3: 为什么Transformer中加入了positional embedding？**
  * 加入位置信息，可以表示序列中token的相对位置。
* **问题4: BERT预训练时mask的比例，可以mask更大的比例吗？**
  * 可，但也不能太大，如原文是mask了15%。除了直接把token改为[mask]，还有随机替换成其他token等处理。
* **问题5: BERT如何进行tokenize操作？有什么好处？**
  * [参考](https://zhuanlan.zhihu.com/p/86965595)：使用subword方法中的Wordpieces，拆词的规则常用Byte Pair Encode；
  * 好处有几点：可以学习到词根词缀的关系；粒度处于word和char之间，权衡了OOV问题
* **问题6: GPT如何进行tokenize操作？和BERT的区别是什么？**
  * 使用Byte Pair Encoding在词汇表中创建token；
  * GPT产生每个token后将此token加到输入的序列中，但只考虑了单向的信息，是自回归的思想。而Bert中一个token要产生self-attention后的输出，需要用到序列中所有的token，即在pre-train时mask掉一部分的token，根据上下文去预测该处真实的单词，是自编码的思想。
* **问题7: BERT模型特别大，单张GPU训练仅仅只能放入1个batch的时候，怎么训练？**
  * 
* **问题8: Transformer为什么需要一个position embedding？**
  * 
* **问题9: Transformer中的残差网络结构作用是什么？**
  * 
* **问题10: BERT训练的时候mask单词的比例可以特别大（大于80%）吗？**
  * 
* **问题11: BERT预训练是如何做mask的？**
  * 
* **问题11: word2vec到BERT改进了什么？**
  * 