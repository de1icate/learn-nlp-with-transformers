## 篇章小测
* **问题1: Transformer中的softmax计算为什么需要除以$d_k$​?**
  * 向量$q,k$点积时会改变数的量级，很可能导致在对softmax优化时出现梯度消失的问题。所以要除以$\sqrt{d_k}$做归一化，可以使反向传播时梯度下降更稳定。
* **问题2: Transformer中attention score计算时候如何mask掉padding位置？**
  * [各种mask](https://zhuanlan.zhihu.com/p/139595546)：[padding]是为了补齐长度的字符，无实际意义，所以要用mask屏蔽掉这块的信息。padding mask：方法是计算$attention\_score=QK^T$​时，令mask区域的输出为负无穷(-1e9)，经过softmax后就为0了。
* **问题3: 为什么Transformer中加入了positional embedding？**
  * 加入位置信息，可以表示序列中token的位置和它们之间的相对距离。
* **问题4: BERT预训练时mask的比例，可以mask更大的比例吗？**
  * 可，但也不能太大，如原文是mask了15%。除了直接把token改为[mask]，还有随机替换成其他token、保持不变的处理。
* **问题5: BERT如何进行tokenize操作？有什么好处？**
  * [subword](https://zhuanlan.zhihu.com/p/86965595)：使用subword方法中的Wordpieces，拆词的规则常用Byte Pair Encode；
  * 好处有几点：可以学习到词根词缀的关系；粒度处于word和char之间，权衡了OOV问题
* **问题6: GPT如何进行tokenize操作？和BERT的区别是什么？**
  * 使用Byte Pair Encoding在词汇表中创建token；
  * GPT产生每个token后将此token加到输入的序列中，但只考虑了单向的信息，是自回归的思想。而Bert中一个token要经过self-attention产生输出，需要用到序列中所有的token，即在pre-train时mask掉一部分的token，根据上下文去预测该处真实的单词，是自编码的思想。
* **问题7: BERT模型特别大，单张GPU训练仅仅只能放入1个batch的时候，怎么训练？**
  * 缩小batch size、多张GPU并行训练 torch.nn.DataParallel。
* **问题8: Transformer为什么需要一个position embedding？**
  * 除了Token embedding，加入Position embedding的信息描述token间的位置关系。
* **问题9: Transformer中的残差网络结构作用是什么？**
  * Transformer的Encoder和Decoder各有6个Layer，引入残差块主要是解决深度神经网络里退化的问题，便于训练更深的网络。
* **问题10: BERT训练的时候mask单词的比例可以特别大（大于80%）吗？**
  * 指的是15%[MASK]里的80%吗？可以，原文设置了不同的mask/same/random比例，发现不同比例的mask策略结果差异不大，即mask策略鲁棒性较好
* **问题11: BERT预训练是如何做mask的？**
  * [Bert问题参考](https://zhuanlan.zhihu.com/p/95594311)：在pre-train阶段的Masked LM任务中，一个batch中mask掉15%的词，其中的80%换为[MASK]，10%随机替换，10%保持不变。由于任何一个非[MASK]的token都可能是80%中被10%随机换掉的，这使得Bert要正确学到上下文信息甚至对上下文进行纠错，才能预测[MASK]处原来的token。
* **问题12: word2vec到BERT改进了什么？**
  * word2vec采用滑动窗口的方式引入上下文信息，并将词语映射到唯一的低维向量。但是对于任一单词只能得到一个固定的词向量表示，对一词多义的问题无能为力；Bert通过设置Masked LM任务，迫使模型做完形填空时真正学到了上下文信息，因此同一个单词也可通过不同的上下文有不同的embedding表示。另一方面，Masked LM任务和word2vec中CBOW的核心思想十分相似，加上Bert使用了Transformer Encoder架构，使用大量的无监督语料，因此获得了SOTA的效果。